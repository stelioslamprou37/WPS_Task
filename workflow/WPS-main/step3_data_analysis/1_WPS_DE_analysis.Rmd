---
title: "WPS DE analysis"
author: "Xuhang Li"
date: "2024-01-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## WPS DE analysis

Although users can choose their favorite Differentially Expression (DE) analysis tool, we recommend using our WPS DE package if ~100 or more conditions are collected (i.e., at least one bacterial plate). When number of conditions is low, please go with regular DE method such as DESeq2 or edgeR. 

This tutorial gives a template to quickly setup a WPS DE analysis with the cleaned dataset processed following our pipeline. 

### Required packages

```{r message=FALSE, warning=FALSE, results='hide'}

# # install WPS DE if not in your environment
# if (!require("devtools", quietly = TRUE)){
#   install.packages("devtools")
# }
# devtools::install_github("XuhangLi/wpsDE")

# load package
library(wpsDE)
library(stringr)
```

### Running WPS DE

WPS DE package is highly integrated with processing WPS dataset. So, running it is straightforward. Let's use met7 data as an example. For running WPS DE, please merge all the data you have if you collect more than one RNAi plate!

```{r}
# load data
load('./../step2_quality_control/outputs/cleaned_merged_raw_data_met7.Rdata')

# prepare metadata table
rep_batch = colnames(input)
rep_batch = str_extract(rep_batch,'rep._met[0-9]+_')
rep_batch = str_replace(rep_batch,'_met._$','')
RNAi = as.character(RNAi)      
RNAi[RNAi == 'x.vector'] = 'control'
batchLabel = as.character(batchLabel)

metaDataTable = data.frame(sampleID = colnames(input),
                           covTreatment = RNAi,
                           covBatch = rep_batch, # WPS DE starts with per-library DE, so the actual                                                     batch covariates are replicate batches!
                           libID = batchLabel
                           )

res = WPS_DE(input, metaDataTable)

# the DE tables are in res. See WPS_DE help document for details.

```
Because of the large scale of the dataset, DE analysis can take a few hours. We will release a parallelization function soon to increase the speed.

